{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_rows = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating the datasets\n",
    "house_data_train = pd.read_csv(\"train.csv\")\n",
    "house_data_test = pd.read_csv(\"test.csv\")\n",
    "# Target prices:\n",
    "y = house_data_train['SalePrice']\n",
    "house_data_train = house_data_train.drop(columns=['SalePrice'])\n",
    "house_data_train.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Values and meanings from the first column, MSSubClass\n",
    "MSSubClass_dict = { \n",
    "        20:\t'1-STORY 1946 & NEWER ALL STYLES', \n",
    "        30:\t'1-STORY 1945 & OLDER', \n",
    "        40:\t'1-STORY W/FINISHED ATTIC ALL AGES',\n",
    "        45:\t'1-1/2 STORY - UNFINISHED ALL AGES', \n",
    "        50:\t'1-1/2 STORY FINISHED ALL AGES', \n",
    "        60:\t'2-STORY 1946 & NEWER', \n",
    "        70:\t'2-STORY 1945 & OLDER', \n",
    "        75:\t'2-1/2 STORY ALL AGES', \n",
    "        80:\t'SPLIT OR MULTI-LEVEL', \n",
    "        85:\t'SPLIT FOYER', \n",
    "        90:\t'DUPLEX - ALL STYLES AND AGES', \n",
    "       120:\t'1-STORY PUD (Planned Unit Development) - 1946 & NEWER', \n",
    "       150:\t'1-1/2 STORY PUD - ALL AGES', \n",
    "       160:\t'2-STORY PUD - 1946 & NEWER', \n",
    "       180:\t'PUD - MULTILEVEL - INCL SPLIT LEV/FOYER', \n",
    "       190:\t'2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n",
    "\n",
    "# Replacing numbers with column names\n",
    "house_data_train['MSSubClass'].replace(MSSubClass_dict, inplace=True)\n",
    "house_data_test['MSSubClass'].replace(MSSubClass_dict, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_columns = list(house_data_train.dtypes[house_data_train.dtypes == np.object].index)\n",
    "numerical_columns = list(house_data_train.dtypes[house_data_train.dtypes != np.object].index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Numerical and categorical dataframes\n",
    "numerical_train = house_data_train[numerical_columns]\n",
    "numerical_test = house_data_test[numerical_columns]\n",
    "categorical_train = house_data_train[categorical_columns]\n",
    "categorical_test = house_data_test[categorical_columns]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Meanings of the Condition1 and Condition2 column values\n",
    "cond = {\n",
    "    'Artery':\t'Adjacent to arterial street',\n",
    "    'Feedr':\t'Adjacent to feeder street',\t\n",
    "    'Norm':\t'Normal',\t\n",
    "    'RRNn':\t'Within 200 of North-South Railroad',\n",
    "    'RRAn':\t'Adjacent to North-South Railroad',\n",
    "    'PosN':\t'Near positive off-site feature--park, greenbelt, etc.',\n",
    "    'PosA':\t'Adjacent to postive off-site feature',\n",
    "    'RRNe':\t'Within 200 of East-West Railroad',\n",
    "    'RRAe':\t'Adjacent to East-West Railroad'\n",
    "}\n",
    "# Getting the full name of the conditions\n",
    "categorical_train['Condition1'].replace(cond, inplace=True)\n",
    "categorical_train['Condition2'].replace(cond, inplace=True)\n",
    "categorical_test['Condition1'].replace(cond, inplace=True)\n",
    "categorical_test['Condition2'].replace(cond, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sum_1hots(df, column1, column2):\n",
    "    '''This function is for getting only one hot encoder for repeated \n",
    "    conditions in some columns in the dataset '''\n",
    "    c1 = pd.get_dummies(df[column1])\n",
    "    c2 = pd.get_dummies(df[column2])\n",
    "    # Sum both one hot encoders. Exploring this data, I found some 2's, that makes \n",
    "    # no sense, so, I replaced them with 1's\n",
    "    c1c2 = c1.add(c2, fill_value=0).replace(2,1)\n",
    "    df = df.drop(columns=[column1, column2])\n",
    "    return df, c1c2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_train, cond_col_train = sum_1hots(categorical_train, 'Condition1', 'Condition2')\n",
    "categorical_train, ext_col_train = sum_1hots(categorical_train, 'Exterior1st', 'Exterior2nd')\n",
    "categorical_test, cond_col_test = sum_1hots(categorical_test, 'Condition1', 'Condition2')\n",
    "categorical_test, ext_col_test = sum_1hots(categorical_test, 'Exterior1st', 'Exterior2nd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling zero values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_train.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Replacing with a string the nan values. While exploring what kind of nan values we have in the data, \n",
    "# is ok to fill with a Not Aplicable condition. (no basement, no pool, etc)\n",
    "categorical_train.fillna('Not Aplicable', inplace=True)\n",
    "categorical_test.fillna('Not Aplicable', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# With the numerical columns, we only have one that is special, the \n",
    "# year when the garage was built, we fill with the average year\n",
    "avg_garage_year = numerical_train['GarageYrBlt'].mean()\n",
    "numerical_train['GarageYrBlt'].fillna(avg_garage_year, inplace=True)\n",
    "numerical_test['GarageYrBlt'].fillna(avg_garage_year, inplace=True)\n",
    "# The other values can be 0\n",
    "numerical_train.fillna(0, inplace=True)\n",
    "numerical_test.fillna(0, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and appending the One Hot Encoders for each categorical column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def drop_concat(df, one_hot, column):\n",
    "    '''This function accepts a dataframe and a column name. It will get the \n",
    "    one hot encoder for categorical column, then, drop it from the original\n",
    "    dataframe and cancatenate the one hot encoder'''\n",
    "    df = df.drop(columns=[column])\n",
    "    df = pd.concat([df, one_hot], axis=1)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def complete_categorical(df1, df2):\n",
    "    '''Sometimes there are values not present in the test and train set, and\n",
    "    in the one hot encoder this result in different number of columns. In \n",
    "    order to have control over it, this function completes those columns with 0'''\n",
    "    cdf1 = df1.columns\n",
    "    cdf2 = df2.columns\n",
    "    for not_in_df2 in cdf1.difference(cdf2):\n",
    "        df2[not_in_df2] = 0\n",
    "    for not_in_df1 in cdf2.difference(cdf1):\n",
    "        df1[not_in_df1] = 0\n",
    "    df1 = df1.reindex(sorted(df1.columns), axis=1)\n",
    "    df2 = df2.reindex(sorted(df2.columns), axis=1)\n",
    "    return df1, df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_train, bsmt_col_train = sum_1hots(categorical_train, 'BsmtFinType1', 'BsmtFinType2')\n",
    "categorical_test, bsmt_col_test = sum_1hots(categorical_test, 'BsmtFinType1', 'BsmtFinType2')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using the custom functions to get completes categorical train and test set\n",
    "for column in categorical_test.columns:\n",
    "    OH_train = pd.get_dummies(categorical_train[column])\n",
    "    OH_test = pd.get_dummies(categorical_test[column])\n",
    "    OH_train, OH_test = complete_categorical(OH_train, OH_test)\n",
    "    categorical_train = drop_concat(categorical_train, OH_train, column)\n",
    "    categorical_test = drop_concat(categorical_test, OH_test, column)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Appending the sum of one hot encoders created previously\n",
    "for sumhot in [bsmt_col_train, cond_col_train, ext_col_train]:\n",
    "    categorical_train = pd.concat([categorical_train, sumhot], axis=1)\n",
    "\n",
    "for sumhot in [bsmt_col_test, cond_col_test, ext_col_test]:\n",
    "    categorical_test = pd.concat([categorical_test, sumhot], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# common1 = categorical_train.columns.difference(categorical_test.columns)\n",
    "# common2 = categorical_test.columns.difference(categorical_train.columns)\n",
    "# categorical_train = categorical_train.drop(columns=common1) \n",
    "# categorical_test = categorical_test.drop(columns=common2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(common2)\n",
    "# print(common1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d1 = categorical_train.columns.difference(categorical_test.columns)\n",
    "d2 = categorical_test.columns.difference(categorical_train.columns)\n",
    "print(d1,d2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(categorical_train.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(categorical_test.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizing the numerical data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import preprocessing\n",
    "#Train set\n",
    "xnumerical_train = numerical_train.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "xnumerical_train_scaled = min_max_scaler.fit_transform(xnumerical_train)\n",
    "\n",
    "# Test set\n",
    "xnumerical_test = numerical_test.values #returns a numpy array\n",
    "xnumerical_test_scaled = min_max_scaler.fit_transform(xnumerical_test)\n",
    "# Filling NaN values with 0\n",
    "xnumerical_test_scaled[np.isnan(xnumerical_test_scaled)] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the whole dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_train = np.append(xnumerical_train_scaled, categorical_train.values, axis=1)\n",
    "processed_test = np.append(xnumerical_test_scaled, categorical_test.values, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlations "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# norm_train = pd.DataFrame(data = processed_train, columns=categorical_train.columns.append(numerical_train.columns))\n",
    "# norm_train = pd.concat([norm_train, y], axis=1)\n",
    "# norm_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# corr_matrix = norm_train.corr()\n",
    "# (corr_matrix['SalePrice']\n",
    "# .sort_values(ascending=False)\n",
    "# .where(((corr_matrix['SalePrice'] < -0.05) | (corr_matrix['SalePrice'] > 0.05)), other=np.nan)\n",
    "# .dropna()\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine Learning Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import svm\n",
    "\n",
    "regr = svm.SVR()\n",
    "regr.fit(processed_train, y.values)\n",
    "predictions_train = regr.predict(processed_train)\n",
    "predictions_svm = regr.predict(processed_test)\n",
    "predictions_svm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.mean(predictions_train / y.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\n",
    "j = 1461\n",
    "with open('predictions.csv', 'w') as p:\n",
    "    writer = csv.writer(p)\n",
    "    writer.writerow(['Id','SalePrice'])\n",
    "    for pred in predictions:\n",
    "        writer.writerow([j,pred])\n",
    "        j += 1\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor(max_iter=5000, learning_rate='adaptive')\n",
    "sgd.fit(processed_train, y.values)\n",
    "predictions_sgd_train = sgd.predict(processed_train)\n",
    "# predictions_sgd = sgd.predict(processed_test)\n",
    "# predictions_sgd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.max(predictions_sgd_train / y.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\n",
    "j = 1461\n",
    "with open('predictions2.csv', 'w') as p:\n",
    "    writer = csv.writer(p)\n",
    "    writer.writerow(['Id','SalePrice'])\n",
    "    for pred in predictions_sgd:\n",
    "        writer.writerow([j,pred])\n",
    "        j += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_columns = list(house_data_train.dtypes[house_data_train.dtypes == np.object])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}